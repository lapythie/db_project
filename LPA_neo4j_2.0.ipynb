{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase, basic_auth\n",
    "\n",
    "import json\n",
    "import gensim\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовый лексикон - это словарь Натальи Лукашевич, Николая Русначенко и др.\n",
    "\n",
    "Он насчитывает более 6000 предикатов, объединенных в 277 тональных фреймов на основе предположения о существовании набора отношений между аргументами предиката (предикаты внутри одного фрейма разделяют один и тот же набор отношений между аргументами).\n",
    "\n",
    "Почему фреймы тональные?\n",
    "Потому что каждое отношение является позитивным или негативным. Вообще эти фреймы потом гипотетически используются для извлечения отношений между сущностями из текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': ['повредить', 'испортить'],\n",
       " 'variants': ['вред',\n",
       "  'вредить',\n",
       "  'вывих',\n",
       "  'вывихивать',\n",
       "  'вывихнуть',\n",
       "  'губить',\n",
       "  'загубить',\n",
       "  'запарывать',\n",
       "  'запороть',\n",
       "  'изувечивать',\n",
       "  'изувечить',\n",
       "  'изуродовать',\n",
       "  'исковеркать',\n",
       "  'искорежить',\n",
       "  'испортить',\n",
       "  'калечить',\n",
       "  'коверкание',\n",
       "  'коверканье',\n",
       "  'коверкать',\n",
       "  'корежить',\n",
       "  'навредить',\n",
       "  'нанесение вреда',\n",
       "  'нанесение травмы',\n",
       "  'нанесение урона',\n",
       "  'нанесение ущерба',\n",
       "  'нанести вред',\n",
       "  'нанести повреждение',\n",
       "  'нанести серьезный удар',\n",
       "  'нанести травму',\n",
       "  'нанести урон',\n",
       "  'нанести ущерб',\n",
       "  'наносить вред',\n",
       "  'наносить повреждение',\n",
       "  'наносить серьезный удар',\n",
       "  'наносить травму',\n",
       "  'наносить урон',\n",
       "  'наносить ущерб',\n",
       "  'напортить',\n",
       "  'перекалечить',\n",
       "  'повредить',\n",
       "  'повреждать',\n",
       "  'повреждение',\n",
       "  'погубить',\n",
       "  'подпортить',\n",
       "  'покалечить',\n",
       "  'попортить',\n",
       "  'портить',\n",
       "  'порча',\n",
       "  'принесение вреда',\n",
       "  'принести вред',\n",
       "  'приносить вред',\n",
       "  'причинение вреда',\n",
       "  'причинение урона',\n",
       "  'причинение ущерба',\n",
       "  'причинить вред',\n",
       "  'причинить урон',\n",
       "  'причинить ущерб',\n",
       "  'причинять вред',\n",
       "  'причинять урон',\n",
       "  'причинять ущерб',\n",
       "  'травмировать',\n",
       "  'увечить',\n",
       "  'урон',\n",
       "  'ущерб'],\n",
       " 'comments': 'близко к «ухудшить» и «нарушить»',\n",
       " 'roles': {'a0': 'кто повредил', 'a1': 'кому повредил', 'a2': 'что повредить'},\n",
       " 'frames': {'polarity': [['a0', 'a1', 'neg', 0.7],\n",
       "   ['a1', 'a0', 'neg', 0.7],\n",
       "   ['a0', 'a2', 'neg', 0.7],\n",
       "   ['a2', 'a2', 'pos', 1.0]],\n",
       "  'effect': [['a1', '-', 1.0], ['a2', '-', 1.0]],\n",
       "  'state': [['a1', 'neg', 1.0]]}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загружаем базовый лексикон\n",
    "with open (\"collection.json\", encoding=\"utf-8\") as f:\n",
    "    base_lexicon = json.load(f)\n",
    "# вот так выглядит типичный тональный фрейм\n",
    "base_lexicon['0_62']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Небольшая подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего вариантов глаголов: 2785\n",
      "Выкинули повторяющиеся варианты, осталось: 2639\n"
     ]
    }
   ],
   "source": [
    "variants = Counter()\n",
    "for frame in base_lexicon:\n",
    "    for var in base_lexicon[frame]['variants']:\n",
    "        if (' ' not in var) and (('INFN' or 'VERB') in morph.parse(var)[0].tag):\n",
    "            variants.update([var])\n",
    "print ('Всего вариантов глаголов:', len(variants))\n",
    "\n",
    "var_list = []\n",
    "for var in variants:\n",
    "    if variants[var] > 1:\n",
    "        pass\n",
    "#         print (var)\n",
    "    else:\n",
    "        var_list.append(var)\n",
    "print ('Выкинули повторяющиеся варианты, осталось:', len(var_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'D:\\\\rusvectores\\\\news_mystem_skipgram_1000_20_2015.bin.gz', binary=True)\n",
    "\n",
    "# get embedding\n",
    "# skipgram_model[verb + '_V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Глаголов в словаре модели: 1528\n"
     ]
    }
   ],
   "source": [
    "# проще нагенерить пар и создавать связи между парами только если их нет\n",
    "skipgram_var_list = [var for var in var_list if var+'_V' in skipgram_model.vocab]\n",
    "print ('Глаголов в словаре модели:', len(skipgram_var_list))\n",
    "\n",
    "# relations_count.value()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эмбеддингов в словаре: 1528\n",
      "Wall time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# сначала наделаем эмбеддингов\n",
    "\n",
    "def make_emb_dict(skipgram_var_list):\n",
    "    emb_dict = {}\n",
    "    for var in skipgram_var_list:\n",
    "        emb = np.atleast_2d(skipgram_model[var + '_V'])\n",
    "        emb_dict.update({var:emb})\n",
    "    print ('Эмбеддингов в словаре:', len(emb_dict))\n",
    "    return emb_dict\n",
    "\n",
    "emb_dict = make_emb_dict(skipgram_var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# в верхнетреугольную матрицу положим близость\n",
    "skipgram_matrix = np.zeros(shape=(len(skipgram_var_list),len(skipgram_var_list)))\n",
    "\n",
    "for i, verb1 in enumerate (skipgram_var_list):\n",
    "    emb1 = emb_dict[verb1]\n",
    "\n",
    "    for j, verb2 in enumerate (skipgram_var_list):\n",
    "        if i < j:\n",
    "            emb2 = emb_dict[verb2]\n",
    "            skipgram_matrix[i, j] = cosine_similarity(emb1, emb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5354784727096558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (str(skipgram_matrix[1526, 1527]))\n",
    "skipgram_matrix[1527, 1526] == skipgram_matrix[1526, 1527]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRUD, Sort, Label propagation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(uri=\"bolt://localhost:7687\", \n",
    "                              auth=basic_auth(\"neo4j\", \"password\"),\n",
    "                             encrypted=False)\n",
    "session = driver.session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем класть в базу по одному фрейму"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE\n",
    "\n",
    "def create_frame(frame, filter_with, base_lexicon=base_lexicon):\n",
    "    '''Parameters:\n",
    "    \n",
    "    frame: str'''\n",
    "    query_count = 0\n",
    "#     складываем уникальные глаголы\n",
    "    for variant in base_lexicon[frame]['variants']:\n",
    "#         здесь нужен список уникальных глаголов\n",
    "        if variant in filter_with:\n",
    "            cypher_query = \"create (p:Predicate {lemma: '\"+variant+\"' , pos:'verb', frame: '\"+frame+\"'})\"\n",
    "            session.run(cypher_query, parameters={})\n",
    "            query_count +=1\n",
    "    \n",
    "#     складываем аргументы\n",
    "    for argument in base_lexicon[frame]['roles'].keys():\n",
    "        description = base_lexicon[frame]['roles'][argument]\n",
    "        cypher_query = \"create (a:Argument:\"+argument+\" {role: '\"+description+\"', frame: '\"+frame+\"'})\"\n",
    "        session.run(cypher_query, parameters={})\n",
    "        query_count +=1\n",
    "\n",
    "    try: # слот polarity есть не в каждом фрейме, так что пытаемся\n",
    "#         пытаемся сложить отношения\n",
    "        for pol in base_lexicon[frame]['frames']['polarity']:\n",
    "            \n",
    "            if pol[0] == 'author':\n",
    "#                 автор добавляется, коль скоро в нем есть потребность\n",
    "                cypher_query = \"merge (a:Argument:\"+pol[0]+\" {frame: '\"+frame+\"'})\"\n",
    "                session.run(cypher_query, parameters={})\n",
    "                query_count +=1\n",
    "\n",
    "            q1 = \"match (x:Argument:\"+pol[0]+\" {frame:'\"+frame+\"'}), (y:Argument:\"+pol[1]+\" {frame:'\"+frame+\"'})\"\n",
    "            q2 = \"merge (x)-[:HAS_ATTITUDE {polarity:'\"+pol[2]+\"', confidence:\"+str(pol[3])+\", frame:'\"+frame+\"'}]->(y)\"\n",
    "            cypher_query = q1+' '+q2\n",
    "            session.run(cypher_query, parameters={})\n",
    "            query_count +=1\n",
    "            \n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    print ('Number of queries:', query_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries: 9\n",
      "Wall time: 37 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CREATE\n",
    "create_frame('0_0', skipgram_var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ\n",
    "# посмотрим, какие глаголы из фрейма легли в базу\n",
    "def read_verbs(frame=False, verbose=True):\n",
    "    if frame:\n",
    "        cypher_query = \"match (p:Predicate) where p.pos = 'verb' \\\n",
    "        and p.frame = '\"+frame+\"' return p.lemma\"\n",
    "    else:\n",
    "        cypher_query = \"match (p:Predicate) where p.pos = 'verb' \\\n",
    "        return p.lemma\"\n",
    "        \n",
    "    results = session.run(cypher_query)\n",
    "    \n",
    "    if verbose:\n",
    "        for res in results.value():\n",
    "            print (res)\n",
    "    else:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "чаять\n",
      "уповать\n",
      "понадеяться\n",
      "надеяться\n",
      "рассчитывать\n",
      "ждать\n",
      "Wall time: 33 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# READ\n",
    "read_verbs('0_0', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries: 5\n",
      "доверяться\n",
      "полагаться\n",
      "Wall time: 345 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "create_frame('0_1', skipgram_var_list)\n",
    "read_verbs('0_1', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries: 14\n",
      "задыхаться\n",
      "отравляться\n",
      "почить\n",
      "скончаться\n",
      "умирать\n",
      "вымирать\n"
     ]
    }
   ],
   "source": [
    "create_frame('0_2', skipgram_var_list)\n",
    "read_verbs('0_2', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE\n",
    "# добавим seed_label\n",
    "# DELETE\n",
    "# удалим seed_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы положили три фрейма. \"Экспертно\" выберем два сидовых глагола, от которых будем распространять лейблы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE\n",
    "\n",
    "def set_seed_labels(seed_verbs, seed_labels, verbose=True):\n",
    "    '''Parameters:\n",
    "    \n",
    "    seed_verbs: list\n",
    "    \n",
    "    seed_labels: list'''\n",
    "    assert len(seed_verbs) == len(seed_labels)\n",
    "    seed_dict = dict(zip(seed_verbs, seed_labels))\n",
    "    for verb in seed_dict:\n",
    "        cypher_query = \"MATCH (n:Predicate) \\\n",
    "        WHERE n.lemma = '\"+verb+\"' AND NOT (EXISTS (n.seed_label)) \\\n",
    "        SET n.seed_label = \"+str(seed_dict[verb])\n",
    "        session.run(cypher_query)\n",
    "        if verbose:\n",
    "            print ('Глагол: '+verb+', лейбл: '+str(seed_dict[verb]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Глагол: надеяться, лейбл: 1\n",
      "Глагол: умирать, лейбл: 2\n"
     ]
    }
   ],
   "source": [
    "# UPDATE\n",
    "seed_verbs = 'надеяться умирать'.split()\n",
    "seed_labels = [1, 2]\n",
    "set_seed_labels(seed_verbs, seed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "# потрем ненужные лейблы\n",
    "def delete_seed_labels(labels=False):\n",
    "    '''Parameters:\n",
    "    \n",
    "    labels: list of labels or False to delete all seed labels\n",
    "    '''\n",
    "    if labels: \n",
    "        for lab in labels:\n",
    "            cypher_query = \"match (n) \\\n",
    "            where n.seed_label = \"+str(lab)+\" \\\n",
    "            remove n.seed_label\"\n",
    "            session.run(cypher_query)\n",
    "    else:\n",
    "        cypher_query = \"MATCH (n) \\\n",
    "        where n.seed_label is not null \\\n",
    "        remove n.seed_label\"\n",
    "        session.run(cypher_query)\n",
    "\n",
    "def delete_frame(frame):\n",
    "    cypher_query = \"MATCH (n) where n.frame = '\"+frame+\"' DETACH DELETE n\"\n",
    "    results = session.run(cypher_query)\n",
    "    for res in results.values():\n",
    "        print (res)\n",
    "    \n",
    "def delete_all():\n",
    "    cypher_query = \"MATCH (n) DETACH DELETE n\"\n",
    "    results = session.run(cypher_query)\n",
    "    for res in results.values():\n",
    "        print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "delete_seed_labels([1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим отношения близости. Используем загруженную выше модель. Отношения будем добавлять только один раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE\n",
    "def relate_nodes(skipgram_model):\n",
    "\n",
    "    query_count = 0\n",
    "\n",
    "    # это лист лежащих в базе глаголов\n",
    "    list2combine = read_verbs(frame=False, verbose=False).value()\n",
    "    # это все возможные пары глаголов\n",
    "    pairs = list(combinations(list2combine, 2))\n",
    "\n",
    "    for verb1, verb2 in pairs:\n",
    "\n",
    "        emb1 = np.atleast_2d(skipgram_model[verb1 + '_V'])\n",
    "        emb2 = np.atleast_2d(skipgram_model[verb2 + '_V'])\n",
    "        cos_sim = str(float(cosine_similarity(emb1, emb2)))\n",
    "\n",
    "        cypher_query = \"match \\\n",
    "        (verb1:Predicate {lemma:'\"+verb1+\"'}), \\\n",
    "        (verb2:Predicate {lemma:'\"+verb2+\"'}) \\\n",
    "        merge (verb1)-[:SIMILAR_TO {skipgram_similarity:\"+cos_sim+\"}]->(verb2)\"\n",
    "#             merge (verb2)-[:SIMILAR_TO {skipgram_similarity:\"+cos_sim+\"}]->(verb1) \\\n",
    "\n",
    "        session.run(cypher_query)\n",
    "        query_count += 1\n",
    "    print ('Запросов в базу:', query_county_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запросов в базу: 91\n",
      "Wall time: 3.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "relate_nodes(skipgram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем присвоить метки другим глаголам на основании семантической близости (label propagation algorithm). Будем использовать ненаправленные отношения с весом, равным косинусной близости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lpa(relationshipProperties, maxIterations=10, verbose=True):\n",
    "    \n",
    "    cypher_query = '''CALL gds.graph.create(\n",
    "    'myGraph',\n",
    "    'Predicate',\n",
    "    'SIMILAR_TO',\n",
    "    {\n",
    "        nodeProperties: 'seed_label',\n",
    "        relationshipProperties: \"'''+relationshipProperties+'''\"\n",
    "    })\n",
    "    '''\n",
    "    session.run(cypher_query)\n",
    "    \n",
    "    cypher_query = '''\n",
    "    CALL gds.labelPropagation.stream('myGraph', \n",
    "    {maxIterations:'''+str(maxIterations)+''', \n",
    "    relationshipWeightProperty: 'skipgram_similarity', \n",
    "    seedProperty: 'seed_label'})\n",
    "    YIELD nodeId, communityId AS Community\n",
    "    RETURN gds.util.asNode(nodeId).lemma AS Lemma, Community\n",
    "    ORDER BY Community, Lemma\n",
    "    '''\n",
    "    lpa_results = session.run(cypher_query)\n",
    "    \n",
    "    try:\n",
    "        cypher_query = '''CALL gds.graph.drop('myGraph') YIELD graphName'''\n",
    "        session.run(cypher_query)\n",
    "    except:\n",
    "        pass\n",
    "    if verbose:\n",
    "        for lemma, community in lpa_results.values():\n",
    "            print(lemma, '\\t\\t', community)\n",
    "    else:\n",
    "        return lpa_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "понадеяться \t\t 1\n",
      "отравляться \t\t 2\n",
      "почить \t\t 2\n",
      "скончаться \t\t 2\n",
      "задыхаться \t\t 4\n",
      "умирать \t\t 43\n",
      "чаять \t\t 44\n",
      "уповать \t\t 45\n",
      "надеяться \t\t 47\n",
      "доверяться \t\t 63\n",
      "вымирать \t\t 68\n",
      "ждать \t\t 68\n",
      "полагаться \t\t 68\n",
      "рассчитывать \t\t 68\n",
      "Wall time: 23 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_lpa('skipgram_similarity', 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "вымирать \t\t 68\n",
      "доверяться \t\t 68\n",
      "ждать \t\t 68\n",
      "задыхаться \t\t 68\n",
      "надеяться \t\t 68\n",
      "отравляться \t\t 68\n",
      "полагаться \t\t 68\n",
      "понадеяться \t\t 68\n",
      "почить \t\t 68\n",
      "рассчитывать \t\t 68\n",
      "скончаться \t\t 68\n",
      "умирать \t\t 68\n",
      "уповать \t\t 68\n",
      "чаять \t\t 68\n",
      "Wall time: 33 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# на 10 итерациях уже одно сплошное коммьюнити\n",
    "run_lpa('skipgram_similarity', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ничего не получилось. Предполагаемые причины:\n",
    "1. Веса связей слишком незначительно отличаются.\n",
    "2. Несбалансированные классы, (в смысле, коммьюнити).\n",
    "3. Мало узлов с предписанными коммьюнити.\n",
    "\n",
    "Дальнейший план действий:\n",
    "* Подбирать итерации на графах с большИм (и бОльшим) количеством узлов с предписанными коммьюнити.\n",
    "* Что-то делать с весами.\n",
    "* Возможно, добавить отношений, используя другую модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Всё заново. Возьмем другие фреймы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries: 14\n",
      "Number of queries: 22\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CREATE\n",
    "create_frame('0_2', skipgram_var_list)\n",
    "create_frame('0_30', skipgram_var_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "аплодировать\n",
      "благословлять\n",
      "возвеличивать\n",
      "восхвалять\n",
      "зааплодировать\n",
      "захлопать\n",
      "нахваливать\n",
      "поаплодировать\n",
      "поощрять\n",
      "похвалить\n",
      "рассыпаться\n",
      "рукоплескать\n",
      "славить\n",
      "хвалить\n",
      "хлопать\n",
      "вымирать\n",
      "задыхаться\n",
      "отравляться\n",
      "почить\n",
      "скончаться\n",
      "умирать\n"
     ]
    }
   ],
   "source": [
    "read_verbs(frame=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Глагол: почить, лейбл: 1\n",
      "Глагол: скончаться, лейбл: 1\n",
      "Глагол: умирать, лейбл: 1\n",
      "Глагол: поощрять, лейбл: 2\n",
      "Глагол: похвалить, лейбл: 2\n",
      "Глагол: благословлять, лейбл: 2\n"
     ]
    }
   ],
   "source": [
    "# UPDATE\n",
    "seed_verbs = '''почить\n",
    "скончаться\n",
    "умирать\n",
    "поощрять\n",
    "похвалить\n",
    "благословлять\n",
    "'''.split()\n",
    "\n",
    "seed_labels = [1,1,1,2,2,2]\n",
    "set_seed_labels(seed_verbs, seed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запросов в базу: 210\n",
      "Wall time: 7.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "relate_nodes(skipgram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "аплодировать \t\t 1\n",
      "благословлять \t\t 1\n",
      "возвеличивать \t\t 1\n",
      "восхвалять \t\t 1\n",
      "вымирать \t\t 1\n",
      "зааплодировать \t\t 1\n",
      "задыхаться \t\t 1\n",
      "захлопать \t\t 1\n",
      "нахваливать \t\t 1\n",
      "отравляться \t\t 1\n",
      "поаплодировать \t\t 1\n",
      "поощрять \t\t 1\n",
      "похвалить \t\t 1\n",
      "почить \t\t 1\n",
      "рассыпаться \t\t 1\n",
      "рукоплескать \t\t 1\n",
      "скончаться \t\t 1\n",
      "славить \t\t 1\n",
      "умирать \t\t 1\n",
      "хвалить \t\t 1\n",
      "хлопать \t\t 1\n",
      "Wall time: 42 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_lpa('skipgram_similarity', 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "благословлять \t\t 1\n",
      "вымирать \t\t 1\n",
      "задыхаться \t\t 1\n",
      "отравляться \t\t 1\n",
      "почить \t\t 1\n",
      "рассыпаться \t\t 1\n",
      "рукоплескать \t\t 1\n",
      "скончаться \t\t 1\n",
      "славить \t\t 1\n",
      "умирать \t\t 1\n",
      "хвалить \t\t 1\n",
      "хлопать \t\t 1\n",
      "аплодировать \t\t 2\n",
      "возвеличивать \t\t 2\n",
      "восхвалять \t\t 2\n",
      "нахваливать \t\t 2\n",
      "поаплодировать \t\t 2\n",
      "зааплодировать \t\t 104\n",
      "поощрять \t\t 112\n",
      "похвалить \t\t 112\n",
      "захлопать \t\t 113\n",
      "Wall time: 35 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_lpa('skipgram_similarity', 1, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Резюме: всё плохо (пока). Не опускаем руки!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
